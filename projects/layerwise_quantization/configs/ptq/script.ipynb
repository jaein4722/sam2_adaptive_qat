{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b5412821",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8148a339",
   "metadata": {},
   "outputs": [],
   "source": [
    "json1 = json.load(open('bit_recommendation_v3_ordered.json'))\n",
    "json2 = json.load(open('bit_recommendation_v4_aggressive.json'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "53609351",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_values([{'image_encoder.neck.convs.0': 4, 'image_encoder.neck.convs.1': 6, 'image_encoder.neck.convs.2': 4, 'image_encoder.neck.convs.3': 4, 'image_encoder.trunk.blocks.0': 7, 'image_encoder.trunk.blocks.1': 6, 'image_encoder.trunk.blocks.2': 7, 'image_encoder.trunk.blocks.3': 6, 'image_encoder.trunk.blocks.4': 6, 'image_encoder.trunk.blocks.5': 7, 'image_encoder.trunk.blocks.6': 6, 'image_encoder.trunk.blocks.7': 6, 'image_encoder.trunk.blocks.8': 6, 'image_encoder.trunk.blocks.9': 6, 'image_encoder.trunk.blocks.10': 6, 'image_encoder.trunk.blocks.11': 6, 'image_encoder.trunk.blocks.12': 7, 'image_encoder.trunk.blocks.13': 6, 'image_encoder.trunk.blocks.14': 6, 'image_encoder.trunk.blocks.15': 6, 'image_encoder.trunk.blocks.16': 6, 'image_encoder.trunk.blocks.17': 6, 'image_encoder.trunk.blocks.18': 6, 'image_encoder.trunk.blocks.19': 6, 'image_encoder.trunk.blocks.20': 6, 'image_encoder.trunk.blocks.21': 4, 'image_encoder.trunk.blocks.22': 4, 'image_encoder.trunk.blocks.23': 4, 'image_encoder.trunk.patch_embed.proj': 8, 'image_encoder.trunk.pos_embed': 4, 'image_encoder.trunk.pos_embed_window': 4, 'memory_attention.layers.0.cross_attn_image': 4, 'memory_attention.layers.0.linear1': 4, 'memory_attention.layers.0.linear2': 4, 'memory_attention.layers.0.norm1': 4, 'memory_attention.layers.0.norm2': 4, 'memory_attention.layers.0.norm3': 4, 'memory_attention.layers.0.self_attn': 4, 'memory_attention.layers.1.cross_attn_image': 4, 'memory_attention.layers.1.linear1': 4, 'memory_attention.layers.1.linear2': 4, 'memory_attention.layers.1.norm1': 4, 'memory_attention.layers.1.norm2': 4, 'memory_attention.layers.1.norm3': 4, 'memory_attention.layers.1.self_attn': 4, 'memory_attention.layers.2.cross_attn_image': 4, 'memory_attention.layers.2.linear1': 4, 'memory_attention.layers.2.linear2': 4, 'memory_attention.layers.2.norm1': 4, 'memory_attention.layers.2.norm2': 4, 'memory_attention.layers.2.norm3': 4, 'memory_attention.layers.2.self_attn': 4, 'memory_attention.layers.3.cross_attn_image': 4, 'memory_attention.layers.3.linear1': 4, 'memory_attention.layers.3.linear2': 4, 'memory_attention.layers.3.norm1': 4, 'memory_attention.layers.3.norm2': 4, 'memory_attention.layers.3.norm3': 4, 'memory_attention.layers.3.self_attn': 4, 'memory_attention.norm.bias': 4, 'memory_attention.norm.weight': 4, 'memory_encoder.fuser.layers.0': 4, 'memory_encoder.fuser.layers.1': 4, 'memory_encoder.mask_downsampler.encoder.0': 6, 'memory_encoder.mask_downsampler.encoder.1': 4, 'memory_encoder.mask_downsampler.encoder.3': 6, 'memory_encoder.mask_downsampler.encoder.4': 4, 'memory_encoder.mask_downsampler.encoder.6': 6, 'memory_encoder.mask_downsampler.encoder.7': 4, 'memory_encoder.mask_downsampler.encoder.9': 4, 'memory_encoder.mask_downsampler.encoder.10': 4, 'memory_encoder.mask_downsampler.encoder.12': 4, 'memory_encoder.out_proj.bias': 4, 'memory_encoder.out_proj.weight': 4, 'memory_encoder.pix_feat_proj.bias': 4, 'memory_encoder.pix_feat_proj.weight': 4, 'sam_mask_decoder.conv_s0.bias': 6, 'sam_mask_decoder.conv_s0.weight': 4, 'sam_mask_decoder.conv_s1.bias': 4, 'sam_mask_decoder.conv_s1.weight': 4, 'sam_mask_decoder.iou_prediction_head.layers.0': 4, 'sam_mask_decoder.iou_prediction_head.layers.1': 4, 'sam_mask_decoder.iou_prediction_head.layers.2': 4, 'sam_mask_decoder.iou_token.weight': 4, 'sam_mask_decoder.mask_tokens.weight': 4, 'sam_mask_decoder.obj_score_token.weight': 4, 'sam_mask_decoder.output_hypernetworks_mlps.0.layers': 4, 'sam_mask_decoder.output_hypernetworks_mlps.1.layers': 4, 'sam_mask_decoder.output_hypernetworks_mlps.2.layers': 4, 'sam_mask_decoder.output_hypernetworks_mlps.3.layers': 4, 'sam_mask_decoder.output_upscaling.0.bias': 4, 'sam_mask_decoder.output_upscaling.0.weight': 4, 'sam_mask_decoder.output_upscaling.1.bias': 4, 'sam_mask_decoder.output_upscaling.1.weight': 4, 'sam_mask_decoder.output_upscaling.3.bias': 6, 'sam_mask_decoder.output_upscaling.3.weight': 4, 'sam_mask_decoder.pred_obj_score_head.layers.0': 4, 'sam_mask_decoder.pred_obj_score_head.layers.1': 4, 'sam_mask_decoder.pred_obj_score_head.layers.2': 6, 'sam_mask_decoder.transformer.final_attn_token_to_image.k_proj': 4, 'sam_mask_decoder.transformer.final_attn_token_to_image.out_proj': 4, 'sam_mask_decoder.transformer.final_attn_token_to_image.q_proj': 4, 'sam_mask_decoder.transformer.final_attn_token_to_image.v_proj': 4, 'sam_mask_decoder.transformer.layers.0': 7, 'sam_mask_decoder.transformer.layers.1': 7, 'sam_mask_decoder.transformer.norm_final_attn.bias': 4, 'sam_mask_decoder.transformer.norm_final_attn.weight': 4, 'sam_prompt_encoder.mask_downscaling.0.bias': 6, 'sam_prompt_encoder.mask_downscaling.0.weight': 6, 'sam_prompt_encoder.mask_downscaling.1.bias': 6, 'sam_prompt_encoder.mask_downscaling.1.weight': 6, 'sam_prompt_encoder.mask_downscaling.3.bias': 6, 'sam_prompt_encoder.mask_downscaling.3.weight': 6, 'sam_prompt_encoder.mask_downscaling.4.bias': 4, 'sam_prompt_encoder.mask_downscaling.4.weight': 4, 'sam_prompt_encoder.mask_downscaling.6.bias': 4, 'sam_prompt_encoder.mask_downscaling.6.weight': 4, 'sam_prompt_encoder.no_mask_embed.weight': 4, 'sam_prompt_encoder.not_a_point_embed.weight': 4, 'sam_prompt_encoder.point_embeddings.0.weight': 4}])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json1.values()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "18959fc4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "dict_keys(['per_layer_bits'])"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "json1.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "01dcc81a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>image_encoder.neck.convs.0</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_encoder.neck.convs.1</th>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_encoder.neck.convs.2</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_encoder.neck.convs.3</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_encoder.trunk.blocks.0</th>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sam_prompt_encoder.mask_downscaling.6.bias</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sam_prompt_encoder.mask_downscaling.6.weight</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sam_prompt_encoder.no_mask_embed.weight</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sam_prompt_encoder.not_a_point_embed.weight</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sam_prompt_encoder.point_embeddings.0.weight</th>\n",
       "      <td>4</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              0\n",
       "image_encoder.neck.convs.0                    4\n",
       "image_encoder.neck.convs.1                    6\n",
       "image_encoder.neck.convs.2                    4\n",
       "image_encoder.neck.convs.3                    4\n",
       "image_encoder.trunk.blocks.0                  7\n",
       "...                                          ..\n",
       "sam_prompt_encoder.mask_downscaling.6.bias    4\n",
       "sam_prompt_encoder.mask_downscaling.6.weight  4\n",
       "sam_prompt_encoder.no_mask_embed.weight       4\n",
       "sam_prompt_encoder.not_a_point_embed.weight   4\n",
       "sam_prompt_encoder.point_embeddings.0.weight  4\n",
       "\n",
       "[120 rows x 1 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1 = pd.DataFrame(json1.values()).T\n",
    "df1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "af47234b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(4.683333333333334)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df1[0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "98d39e0a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>image_encoder.neck.convs.0</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_encoder.neck.convs.1</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_encoder.neck.convs.2</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_encoder.neck.convs.3</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>image_encoder.trunk.blocks.0</th>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sam_prompt_encoder.mask_downscaling.6.bias</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sam_prompt_encoder.mask_downscaling.6.weight</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sam_prompt_encoder.no_mask_embed.weight</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sam_prompt_encoder.not_a_point_embed.weight</th>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>sam_prompt_encoder.point_embeddings.0.weight</th>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>120 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              0\n",
       "image_encoder.neck.convs.0                    3\n",
       "image_encoder.neck.convs.1                    3\n",
       "image_encoder.neck.convs.2                    3\n",
       "image_encoder.neck.convs.3                    3\n",
       "image_encoder.trunk.blocks.0                  8\n",
       "...                                          ..\n",
       "sam_prompt_encoder.mask_downscaling.6.bias    3\n",
       "sam_prompt_encoder.mask_downscaling.6.weight  3\n",
       "sam_prompt_encoder.no_mask_embed.weight       3\n",
       "sam_prompt_encoder.not_a_point_embed.weight   2\n",
       "sam_prompt_encoder.point_embeddings.0.weight  3\n",
       "\n",
       "[120 rows x 1 columns]"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2 = pd.DataFrame(json2.values()).T\n",
    "df2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "a15e4648",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "np.float64(3.341666666666667)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df2[0].mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65547eed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "SAM2",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
